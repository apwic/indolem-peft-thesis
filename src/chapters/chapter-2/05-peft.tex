\section{\textit{Parameter-Efficient Fine-Tuning}}
\label{sec:peft}

\textit{Fine-tuning} dari \textit{pre-trained model} terbukti efektif untuk berbagai tugas NLP \parencite{fine_tuning}. Namun, melakukan \textit{fine-tuning} pada seluruh model termasuk tidak efektif secara parameter karena akan menghasilkan model baru untuk setiap tugas yang dilatih. Apalagi dengan jumlah parameter yang mencapai jutaan bahkan miliaran, model yang dihasilkan dari \textit{fine-tuning} akan sangat tidak efisien. Banyak penelitian mengajukan teknik \textit{fine-tuning} yang efisien secara parameter, yaitu \PEFT dengan hanya melakukan \textit{tuning} pada sebagian dari parameter model.

\subsection{\textit{Adapter}}

\textit{Bottleneck adapter} merupakan modul \textit{adapter} dengan menggunakan arsitektur \textit{bottleneck} \parencite{adapter_houlsby}. Konsep \textit{bottleneck adapter} diajukan sebagai alternatif dari \textit{fine-tuning}, dengan tiga properti utama, yaitu kinerja yang baik, pelatihan yang sekuensial (tidak perlu akses secara simultan terhadap keseluruhan \textit{dataset}), dan hanya menambahkan sebagian kecil parameter untuk setiap tugas \parencite{adapter_houlsby}. Strategi \textit{fine-tuning} dengan \textit{adapter} dilakukan dengan memasukkan \textit{layer} tambahan pada arsitektur \textit{Transformer} seperti yang bisa dilihat pada gambar \ref{fig:adapters_houlsby_arch}. Seperti yang telah dibahas pada subbab \ref{sec:transformer}, setiap \textit{layer} pada \textit{Transformer} mempunyai dua \textit{sub-layer}, yaitu \textit{multi-head attention} dan \textit{feed-forward}. Metode \textit{bottleneck adapter} ini akan menambahkan modul \textit{adapter} ke setiap \textit{sub-layer} pada \textit{Transformer} tersebut \parencite{adapter_houlsby}.

\begin{figure}[h]
    \vspace{0.25cm}
    \centering
    \includegraphics[width=\textwidth]{chapter-2/adapter_houlsby_arch.png}
    \caption{Arsitektur dari \textit{Bottleneck Adapter} oleh \citeauthor{adapter_houlsby} \parencite{adapter_houlsby}}
    \label{fig:adapters_houlsby_arch}
\end{figure}

Arsitektur \textit{bottleneck} dari modul \textit{adapter} diajukan untuk membatasi jumlah dari parameter \parencite{adapter_houlsby}. Berdasarkan gambar \ref{fig:adapters_houlsby_arch}, \textit{adapter} melakukan proyeksi dari dimensi asli $d$ ke dimensi yang lebih kecil $m$, lalu dilanjutkan dengan fungsi nonlinear, dan dilakukan proyeksi kembali dari dimensi $m$ ke dimensi asli $d$. Sehingga, total parameter yang ditambahkan untuk setiap \textit{layer} adalah $2md$ yang berasal dari bobot pada kedua proyeksi ditambah dengan $m+d$ yang merupakan biasnya. Pada penggunaannya, dengan memakai nilai $m << d$, parameter yang digunakan adalah sekitar $0,5-8\%$ dari parameter asli model \parencite{adapter_houlsby}.

\begin{equation}
    reduction\_factor = \frac{d_{hidden}}{d_{bottleneck}}
    \label{eq:reduction_factor}
\end{equation}

Parameter yang digunakan dibatasi dengan menggunakan $reduction\_factor$ \parencite{adapterhub}. Nilai dari $reduction\_factor$ tersebut didapat dari rasio antara dimensi asli dengan dimensi yang lebih kecil bisa dilihat pada rumus \ref{eq:reduction_factor} \parencite{adapterhub}. Nilai $d_{hidden}$ merupakan nilai dari dimensi asli $d$. Sedangkan, nilai $d_{bottleneck}$ merupakan dimensi dari \textit{bottleneck} yang lebih kecil dari dimensi aslinya.

Untuk menghitung parameter yang bisa dilatih pada metode \textit{adapter}, perlu ditentukan dimensi dari \textit{bottleneck} dari rumus \ref{eq:reduction_factor}. Nilai dari dimensi $d_{bottleneck}$ akan digunakan untuk menghitung proyeksi matriks. Perhitungan untuk parameter pelatihan dapat dilihat pada rumus \ref{eq:parameter-adapter}. Nilai $\psi$ merupakan nilai parameter pelatihan pada metode PEFT mengikuti konvensi dari \citeauthor{adapter_houlsby} yang disimplifikasi. Nilai matriks dari proyeksi tersebut adalah perkalian dari dimensinya. Lalu, pengali $block$ pada rumus \ref{eq:parameter-adapter} berasal dari banyaknya \textit{layer}. Untuk model berbasis BERT, nilai dari $block$ adalah 12 yang berasal dari jumlah \textit{feed-forward layer} pada \textit{encoder}. Sedangkan, untuk model berbasis T5, nilai dari $block$ adalah 24 karena model \textit{encoder decoder}.

\begin{equation}
    \begin{aligned}
        d_{bottleneck} = reduction\_factor \times {d_{hidden}} \\
        W_{down} = W_{up} = d_{hidden} \times d_{bottleneck} \\
        \psi = (W_{down} + b_{down} + W_{up} + b_{up}) \times block \times forward\_head
    \end{aligned}
    \label{eq:parameter-adapter}
\end{equation}

Penggunaan \textit{adapter} tidak hanya dengan menambahkannya pada setiap \textit{sub-layer Transformer}. Terdapat beberapa konfigurasi yang bisa digunakan. Salah satunya adalah seperti pada gambar \ref{fig:adapters_houlsby_arch} oleh \parencite{adapter_houlsby} dengan menambahkan modul \textit{adapter} pada kedua \textit{sub-layer} (\textit{multi-head attention} dan \textit{feed-forward}). \citeauthor{adapter_pfeiffer} menggunakan \textit{adapter} hanya pada \textit{sub-layer feed-forward}. Sedangkan, \citeauthor{uvpl} menggunakan \textit{adapter} secara paralel pada \textit{layer Transformer}.

\begin{table}[h]
    \vspace{0.25cm}
    \centering
    \caption{Hasil evaluasi \textit{Adapter} konfigurasi \citeauthor{adapter_houlsby} pada GLUE \parencite{adapter_houlsby}}
    \label{table:adapter_houlsby_result}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|p{2cm}|p{2cm}|ccccccccc|c}
        \toprule
        & Total num params & Trained params / task & CoLA & SST & MRPC & STS-B & QQP & MNLI$_m$ & MNLI$_{mm}$ & QNLI & RTE & Total \\
        \midrule
        BERT$_{LARGE}$ & $9,0\times$ & $100\%$ & $60,5$ & $94,9$ & $89,3$ & $87,6$ & $72,1$ & $86,7$ & $85,9$ & $91,1$ & $70,1$ & $80,4$ \\
        Adapters (8-256) & $1,3\times$ & $3,6\%$ & $59,5$ & $94,0$ & $89,5$ & $86,9$ & $71,8$ & $84,9$ & $85,1$ & $90,7$ & $71,5$ & $80,0$ \\
        Adapters (64) & $1,2\times$ & $2,1\%$ & $56,9$ & $94,2$ & $89,6$ & $87,3$ & $71,8$ & $85,3$ & $84,6$ & $91,4$ & $68,8$ & $79,6$ \\
        \bottomrule
    \end{tabular}}
\end{table}

Tabel \ref{table:adapter_houlsby_result} menunjukkan hasil evaluasi dari \textit{Adapter} dengan \textit{baseline} yang menggunakan \textit{fine-tuning} pada kakas evaluasi GLUE. \textit{Adapter} berhasil mendapatkan hasil rata-rata dengan nilai $80,0$, dibandingkan dengan \textit{fine-tuning} yang mendapatkan nilai $80,4$. Selain itu, untuk ukuran dari \textit{adapter} yang tetap yaitu dengan ukuran 64, didapatkan hasil dengan nilai $79,6$. Berdasarkan paparan tersebut, didapatkan bahwa \textit{Adapter} mampu menyaingi kinerja \textit{fine-tuning} dengan hanya menggunakan $3,6\%$ parameter. 

\subsection{\textit{Prefix-Tuning}}
\label{sec:prefix-tuning}

\textit{Prefix-Tuning} merupakan salah satu metode PEFT yang menjadi alternatif dari metode \textit{fine-tuning} yang menggunakan vektor kontinu (disebut dengan \textit{prefix}) \parencite{prefix_tuning}. Metode ini mengambil inspirasi dari \textit{prompting}, memperbolehkan token-token berikutnya untuk memperhatikan \textit{prefix} tersebut seolah-olah \textit{prefix} tersebut adalah \textit{virtual token} \parencite{prefix_tuning}. Mirip dengan \textit{Adapter}, \textit{prefix-tuning} hanya perlu menyimpan sedikit parameter tambahan untuk suatu tugas dengan melakukan \textit{freeze} pada parameter asli model dan hanya mengubah parameter dari \textit{prefix} tersebut. \textit{Prefix-Tuning} yang diajukan oleh \citeauthor{prefix_tuning} dibuat untuk tugas \textit{generation}, sehingga pada implementasi yang dilakukan oleh \citeauthor{adapters}, diperlukan adanya adaptasi untuk tugas evaluasi secara umum. Pada konteks tugas akhir ini, digunakan \textit{Prefix-Tuning} dengan adaptasi dari \citeauthor{adapters} tersebut. 

\begin{figure}[h]
    \vspace{0.25cm}
    \centering
    \includegraphics[width=\textwidth]{chapter-2/prefix_tuning_example.png}
    \caption{Contoh \textit{Prefix-Tuning} pada model \textit{encoder decoder} \parencite{prefix_tuning}}
    \label{fig:prefix_tuning_example}
\end{figure}

Dengan intuisi yang didapatkan dari penggunaan \textit{prompting}, penambahan konteks dapat mengarahkan model tanpa mengubah parameter \parencite{prefix_tuning}. \textit{Prefix-Tuning} menambahkan \textit{prefix} pada kedua \textit{encoder} dan \textit{decoder} untuk mendapatkan $z = [PREFIX;x;PREFIX';y]$ seperti yang bisa dilihat pada gambar \ref{fig:prefix_tuning_example}. $P_{idx}$ merupakan indeks dari \textit{prefix} tersebut. Matriks yang bisa dilatih, diinisialisasi sebagai matriks $P_\theta$ dengan $\theta$ sebagai parameternya. Dimensi dari matriks $P_\theta$ adalah panjang dari $P_{idx}$ dikalikan dengan besar dimensi \textit{activation layer}-nya yaitu $h$. Secara langsung melakukan optimasi terhadap $P_\theta$ itu tidak stabil dan menghasilkan penurunan terhadap kinerja \parencite{prefix_tuning}. Sehingga, matriks $P_\theta$ diparameterisasi sebagai $P_\theta = MLP_\theta(P'_\theta)$ dengan matriks dimensi lebih kecil ($P'_\theta$) dimasukkan ke dalam \textit{feed forward} ($MLP_\theta$).

\begin{figure}[h]
    \vspace{0.25cm}
    \centering
    \includegraphics[height=0.4\textwidth]{chapter-2/prefix_tuning_arch.png}
    \caption{Arsitektur \textit{Prefix-Tuning} \parencite{adapterhub}}
    \label{fig:prefix_tuning_arch}
\end{figure}

Pada gambar \ref{fig:prefix_tuning_arch} ditunjukkan arsitektur dari \textit{Prefix-Tuning} dengan menambahkan \textit{prefix} pada \textit{multi-head attention layer}. \textit{Prefix} ditambahkan pada bagian \textit{key} dan \textit{value} pada \textit{attention head}, menjadi $P^K$ dan $P^V$. Sehingga, rumus \ref{eq:attention} akan menggunakan $[P^K, K]$ dan $[P^V, V]$ menjadi $Attention(Q, [P^K, K], [P^V, V])$ \parencite{adapterhub}. Selain itu, panjang dari \textit{prefix} dapat divariasikan berdasarkan pada nilai $prefix\_length$-nya. 

Parameter pelatihan pada metode \textit{Prefix-Tuning} bergantung pada nilai $prefix\_length$. Untuk menghitung total dari parameter pelatihan, metode \textit{Prefix-Tuning} dibagi menjadi dua komponen, yaitu bagian \textit{embedding} dan \textit{control trans}-nya \parencite{adapters}. Bagian \textit{embedding} ini berisi perkalian antara $prefix\_length$ dengan $d_{hidden}$. Sedangkan, bagian \textit{control trans} berisi pemrosesan dari \textit{prefix} dengan \textit{feed-forward}. Perhitungannya bisa dilihat pada rumus \ref{eq:parameter-prefix-tuning}. Rumus untuk $Linear1$ tidak dikalikan dengan $block$ karena digunakan untuk memproses luaran dari bagian \textit{embedding}. Sedangkan, $Linear2$ dikalikan dengan $2\times{block}$ karena \textit{prefix} terdapat pada bagian \textit{key} dan \textit{value} dari \textit{Transformer}. Di akhir, nilai total dari setiap komponen dikalikan sebanyak \textit{attention head} pada model.

\begin{equation}
    \begin{aligned}
        Embedding = prefix\_length \times d_{hidden} \\
        Linear1 = d_{hidden} \times intermediate\_size + b_{intermediate} \\
        Linear2 = (intermediate\_size \times d_{hidden} + b_{hidden}) \times 2 \times block \\
        \psi = (Embedding + Linear1 + Linear2) \times attention\_head
    \end{aligned}
    \label{eq:parameter-prefix-tuning}
\end{equation}

\begin{table}[h]
    \vspace{0.25cm}
    \centering
    \caption{Hasil evaluasi \textit{Prefix-Tuning} pada tugas \textit{summarization} XSUM \parencite{prefix_tuning}}
    \label{table:prefix_tuning_result}
    \begin{tabular}{lccc}
        \toprule
        Method & R-1 & R-2 & R-L \\
        \midrule
        FINE-TUNE & $45,14$ & $22,27$ & $37,25$ \\
        PREFIX($2\%$) & $43,80$ & $20,93$ & $36,05$ \\
        PREFIX($0,1\%$) & $42,92$ & $20,03$ & $35,05$ \\
        \bottomrule
    \end{tabular}
\end{table}

Pada penelitian yang dilakukan oleh \citeauthor{prefix_tuning}, terdapat dua tugas yang dievaluasi yaitu \textit{table-to-text generation} dan \textit{summarization}. Seperti yang bisa dilihat pada tabel \ref{table:prefix_tuning_result}, dengan hanya $2\%$ parameter dibanding parameter aslinya, \textit{Prefix-Tuning} berhasil mendapatkan kinerja yang sedikit lebih buruk dibanding \textit{fine-tuning} ($36,05$ dibanding $37,25$ pada R-L) \parencite{prefix_tuning}. Hasil ini sedikit berbeda dibanding pada tugas \textit{table-to-text generation} yang mendapatkan hasil yang menyaingi \textit{fine-tuning}. Perbedaan antara kedua tugas tersebut adalah bahwa masukan dari artikel pada XSUM lebih banyak sekitar 17 kali dibanding pada \textit{dataset table-to-text}, serta tugas \textit{summarization} lebih kompleks dibanding \textit{table-to-text generation} karena perlu pemahaman terhadap tulisan dan mengidentifikasi topik penting dari sebuah artikel \parencite{prefix_tuning}.

\subsection{\textit{Low Rank Adaptation} (LoRA)}

\textit{Low Rank Adaptation} atau biasa disebut sebagai LoRA merupakan salah satu metode PEFT. Berbeda dengan metode PEFT yang dibahas pada subbab sebelumnya, penambahan \textit{adapter} seperti yang dilakukan oleh \citeauthor{adapter_houlsby} akan menambahkan waktu inferensi \parencite{lora}. Selain itu, melakukan optimasi pada masukan dengan menggunakan "\textit{prompt}" seperti yang dilakukan oleh \citeauthor{prefix_tuning} membuatnya sulit untuk dioptimasi dan kinerjanya juga tidak selalu meningkat \parencite{lora}. Penambahan panjang dari sekuens masukan akan mengurangi sekuens yang seharusnya bisa digunakan untuk suatu tugas \parencite{lora}. Sehingga, metode tersebut sering kali gagal dalam menyaingi kinerja dari \textit{fine-tuning}, bahkan memerlukan adanya \textit{trade off} antara kinerja dengan efisiensi \parencite{lora}.

\begin{figure}[h]
    \vspace{0.25cm}
    \centering
    \includegraphics[width=0.6\textwidth]{chapter-2/lora.png}
    \caption{Arsitektur LoRA \parencite{lora}}
    \label{fig:lora_arch}
\end{figure}

LoRA menggunakan pendekatan bahwa parameter dari model yang dilatih sebenarnya berada pada dimensi intrinsik yang rendah dan masih dapat belajar secara efisien pada dimensi yang lebih rendah tersebut \parencite{lora}. Untuk matriks beban $W_0$ dari sebuah \textit{pre-trained model} dengan dimensi $d\times{k}$, dekomposisi dengan \textit{rank} $r$ (\textit{low-rank decomposition}) dapat direpresentasikan sebagai $W_0 + \Delta{W} = W_0 + BA$, dengan dimensi $B$ adalah $d\times{r}$ dan dimensi $A$ adalah $r\times{k}$, dan \textit{rank} $r << min(d,k)$ \parencite{lora}. Nilai dari $\Delta(W)$ disesuaikan dengan $\frac{\alpha}{r}$, dengan nilai $\alpha$ adalah konstanta \parencite{lora}. Sehingga, rumus untuk melakukan perhitungan pada \textit{hidden layer} dapat dilihat pada rumus \ref{eq:lora-pass}.

\begin{equation}
    h = W_{0}x + \frac{\alpha}{r}\Delta{W_x} = W_{0}x + \frac{\alpha}{r}BAx
    \label{eq:lora-pass}
\end{equation}

Bisa dilihat pada gambar \ref{fig:lora_arch}, LoRA menggunakan matriks dengan rank yang lebih rendah pada matriks $A$ dan matriks $B$. Matriks $A$ diinisialisasi dengan gaussian, sedangkan matriks $B$ diinisialisasi dengan 0. Sehingga, pada awal pelatihan nilai dari $\Delta{W}=BA$ adalah 0. Ketika proses pelatihan, matriks beban $W_0$ dilakukan \textit{freeze}, sedangkan mariks $B$ dan $A$ adalah parameter yang dilatih \parencite{lora}.

LoRA menggunakan $rank$ sebagai besar dari matriks dekomposisi yang dibuatnya. Perhitungan untuk parameter pelatihan untuk LoRA tergantung dari $d_{hidden}$ dan $rank$-nya. Bisa dilihat pada rumus \ref{eq:parameter-lora}, digunakan pengali $2 \times block$ karena LoRA ditambahkan pada bagian \textit{query} dan \textit{value} berdasarkan implementasi dari \citeauthor{adapters}.

\begin{equation}
    \begin{aligned}
        \psi = ((2 \times d_{hidden} \times rank) \times 2 \times block) \times attention\_head
    \end{aligned}
    \label{eq:parameter-lora}
\end{equation}

\begin{table}[ht]
    \vspace{0.25cm}
    \centering
    \caption{Hasil evaluasi LoRA dengan model GPT-3 \parencite{lora}}
    \label{table:lora_result}
    \begin{tabular}{l|l|ccccc}
        \toprule
        \multirow{2}{*}{Method} & \#Trainable & WikiSQL & MNLI-m & \multicolumn{3}{c}{SAMSum} \\
         & Parameters & \textbf{Acc. (\%)} & \textbf{Acc. (\%)} & \textbf{R1} & \textbf{R2} & \textbf{RL} \\
        \midrule
        FT & 175.255,8M & \textbf{73,8} & 89,5 & 52,0 & 28,0 & 44,5 \\
        BitFit & 14,2M & 71,3 & 91,0 & 51,3 & 27,4 & 44,5 \\
        PreEmbed & 3,2M & 63,1 & 88,6 & 48,4 & 24,2 & 40,5 \\
        PreLayer & 20,2M & 70,1 & 89,5 & 50,8 & 27,3 & 44,5 \\
        Adapter\textsubscript{H}  & 7,1M & 71,9 & 89,8 & 53,0 & 28,9 & 44,9 \\
        Adapter\textsubscript{L}  & 40,1M & 73,2 & \textbf{91,5} & 53,2 & 29,0 & 45,1 \\
        LoRA  & 4,7M & 73,4 & 91,7 & \textbf{53,8} & \textbf{29,8} & \textbf{45,9} \\
        LoRA  & 37,7M & \textbf{74,0} & 91,6 & 53,4 & 29,2 & 45,1 \\
        \bottomrule
    \end{tabular}
\end{table}

Hasil evaluasi LoRA pada beberapa tugas evaluasi dapat dilihat pada tabel \ref{table:lora_result}. LoRA mampu menyaingi atau bahkan melampaui kinerja \textit{fine-tuning} pada ketiga tugas evaluasi tersebut. Selain itu, dari dibandingkan metode PEFT yang lain, LoRA menghasilkan skalabilitas dan kinerja yang lebih baik \parencite{lora}.

\subsection{UniPELT}

Setiap metode PEFT yang yang telah dijelaskan pada subbab-subbab sebelumnya mempunyai karakteristik yang beragama dan memiliki kinerja yang cukup berbeda pada tugas yang sama \parencite{unipelt}. Sehingga, pemilihan metode PEFT yang paling tepat untuk tugas tertentu menjadi tugas yang cukup sulit, terutama dengan jumlah metode PEFT yang terus bertambah \parencite{unipelt}. UniPELT hadir sebagai \textit{framework} yang menyatukan beberapa metode PEFT sebagai submodul dan mencoba belajar untuk menggunakan metode PEFT yang terbaik pada suatu tugas \parencite{unipelt}.

UniPELT menggunakan empat metode yang cukup merepresentasikan banyakanya metode PEFT, yaitu \textit{Adapter}, \textit{Prefix-Tuning}, LoRA, dan BitFit \parencite{unipelt}. Semua metode tersebut sudah dibahas pada subbab sebelumnya, kecuali BitFit. BitFit merupakan metode PEFT yang hanya melakukan pelatihan pada bias dari modelnya saja \parencite{unipelt}. Berdasarkan eksperimen yang dilakukan oleh \citeauthor{unipelt}, BitFit tidak pernah mendapatkan hasil yang terbaik dan secara umum mendapatkan hasil yang terburuk, sehingga BitFit tidak dimasukkan ke dalam metode gabungan dari UniPELT dan hanya digunakan sebagai pembanding pada eksperimen \parencite{unipelt}.

\begin{figure}[h]
    \vspace{0.25cm}
    \centering
    \includegraphics[height=0.4\textheight]{chapter-2/unipelt_arch.png}
    \caption{Arsitektur UniPELT \parencite{unipelt}}
    \label{fig:unipelt_arch}
\end{figure}

Setiap metode PEFT menggunakan bagian yang berbeda pada arsitektur model, sebagai contoh sebelum \textit{multi-head attention} untuk \textit{Prefix-Tuning} dan setelah \textit{feed-forward} untuk \textit{Adapter} \parencite{unipelt}. Sehingga, memungkinkan untuk menggabungkan beberapa metode PEFT tersebut tanpa mengganggu antara satu metode dengan yang lain \parencite{unipelt}. Arsitektur dari gabungan metode PEFT tersebut dapat dilihat pada gambar \ref{fig:unipelt_arch}. Selain menggabungkan, terdapat \textit{gating mechanism} yang akan melakukan aktivasi atau deaktivasi tergantung dari signifikasi kinerja setiap metode PEFT pada tugas yang sedang dilatih \parencite{unipelt}.

\citeauthor{unipelt} melakukan eksperimen dengan \textit{low-resource} dan \textit{high-resource} pada evaluasi GLUE dengan menggunakan model BERT \parencite{unipelt}. Metode UniPELT divariasikan menjadi dua yaitu UniPELT (AP) dan UniPEL (APL), dengan A berarti \textit{Adapter}, P berarti \textit{Prefix-Tuning}, dan \textit{L} berarti LoRA. Secara umum, UniPELT (AP) dan UniPELT (APL) secara konsisten mendapatkan hasil yang terbaik atau kedua terbaik pada rata-rata evaluasi GLUE untuk semua eksperimen \textit{low-resource} \parencite{unipelt}. Kinerja UniPELT lebih baik dibanding dengan masing-masing submodulnya upada eksperimen \textit{low-resource}, yang menunjukkan bahwa UniPELT menghasilkan kinerja yang baik untuk eksperimen tersebut \parencite{unipelt}.

Perhitungan parameter untuk UniPELT merupakan total dari metode sebelumnya. Namun, terdapat \textit{gating mechanism} yang merupakan fungsi linear untuk melakukan aktivasi atau deaktivasi pada setiap metode PEFT terkait. Rumus \ref{eq:parameter-prefix-tuning} menunjukkan perhitungan untuk parameter pelatih dengan $A$, $P$, dan $L$ adalah metode \textit{Adapter}, \textit{Prefix-Tuning}, dan LoRA. Seperti yang bisa dilihat pada rumus \ref{eq:parameter-prefix-tuning}, $gating$ merupakan fungsi linear yang terkait terhadap $d_{hidden}$. Terdapat perbedaan dari pengali pada $gating_{P}$ dari rumus \ref{table:param-model}, modul \textit{prefix} dianggap sebagai 1 untuk kedua $key$ dan $value$.

\begin{equation}
    \begin{aligned}
        gating = d_{hidden} \times 1 + 1 \\
        gating_{A} = gating \times block \times forward\_head \\
        gating_{P} = gating \times block \times attention\_head \\
        gating_{L} = gating \times 2 \times block \times attention\_head \\
        total\_gating = (3 \times attention\_head + forward\_head) \times gating \times block \\
        \psi = \psi_A + \psi_P + \psi_L + total\_gating
    \end{aligned}
    \label{eq:parameter-unipelt}
\end{equation}

\begin{table}[ht]
    \vspace{0.25cm}
    \centering
    \caption{Hasil evaluasi UniPELT pada \textit{high-resource} \parencite{unipelt}}
    \label{table:unipelt_result}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|ccccccccccc}
            \toprule
            Method & SST-2 & MRPC & CoLA & RTE & QNLI & STS-B & MNLI & QQP & Avg. \\
            \midrule
            Fine-tuning & 91,63 & 90,94 & 62,08 & 66,43 & 89,95 & \textbf{89,76} & 83,23 & \textbf{87,35} & 82,67 \\
            Adapter & \textbf{91,86} & 89,86 & 61,51 & 71,84 & 90,55 & 88,63 & 83,14 & 86,78 & 83,02 \\
            Prefix-tuning & 90,94 & \textbf{91,29} & 55,37 & \textbf{76,90} & 90,39 & 87,19 & 81,83 & 82,30 & 82,07 \\
            LoRA & 91,51 & 90,03 & 60,47 & 71,48 & 90,73 & 85,65 & 85,61 & 85,98 & 82,20 \\
            \midrule
            UNIPELT (AP) & \textbf{91,86} & 90,28 & 61,51 & 71,84 & \textbf{90,77} & \textbf{88,76} & 83,12 & 86,78 & 83,02 \\
            -NoGate & 91,74 & 90,18 & 58,63 & 71,12 & 90,38 & 88,76 & 81,54 & 85,83 & 82,23 \\
            UNIPELT (APL) & 91,51 & 90,94 & 61,53 & 73,65 & 90,50 & 88,93 & \textbf{83,89} & 87,12 & \textbf{83,50} \\
            \bottomrule
        \end{tabular}
    }
\end{table}

Pada tabel \ref{table:unipelt_result} merupakan hasil evaluasi dari UniPELT pada \textit{high-resource}, UniPELT mendapatkan hasil yang terbaik secara umum. Peningkatan yang didapatkan tidak terlalu signifikan dibandingkan dengan eksperimen pada \textit{low-resource}, hal ini sesuai karena metode PEFT mampu menyaingi \textit{fine-tuning} pada \textit{dataset} yang banyak \parencite{unipelt}. Hasil UniPELT tanpa dilakukan \textit{gating mechanism} (-NoGate) tidak menghasilkan kinerja yang baik \parencite{unipelt}. Walaupun dengan kinerja yang baik, \citeauthor{unipelt} menyebutkan bahwa penggunaan UniPELT tidak akan selalu menghasilkan hasil yang lebih baik dibandingkan dengan penggunaan submodulnya secara individual, bahkan bisa memberikan hasil yang lebih buruk \parencite{unipelt}.

\begin{table}[ht]
    \vspace{0.25cm}
    \centering
    \caption{Waktu pelatihan dan inferensi dari setiap metode PEFT \parencite{unipelt}}
    \label{table:runtime-peft}
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Method} & \textbf{\#Param.} & \textbf{Time$_T$} & \textbf{Time$_I$} \\ 
        \midrule
        Fine-tuning & 110M (100\%) & 100\% & 100\% \\ 
        BitFit & 103K (0,09\%) & 65\% & 102\% \\ 
        Prefix-tuning & 184K (0,17\%) & 56\% & 114\% \\ 
        LoRA & 295K (0,27\%) & 53\% & 105\% \\ 
        Adapter & 895K (0,81\%) & 55\% & 107\% \\ 
        UNIPELT (AP) & 1,1M (0,99\%) & 55\% & 118\% \\ 
        UNIPELT (APL) & 1,4M (1,26\%) & 67\% & 127\% \\ 
        \bottomrule
    \end{tabular}
\end{table}

Pada penelitian yang dilakukan oleh \citeauthor{unipelt}, dilakukan juga evaluasi terhadap waktu pelatihan dan waktu inferensi antara masing-masing metode PEFT, seperti yang bisa dilihat pada tabel \ref{table:runtime-peft}. Waktu pelatihan yang didapatkan dengan metode PEFT lebih cepat dibandingkan dengan \textit{fine-tuning} karena parameter yang digunakan untuk pelatihan juga lebih sedikit \parencite{unipelt}. Untuk waktu inferensi, semuanya menjadi lebih besar dibandingkan dengan \textit{fine-tuning} karena metode PEFT membutuhkan \textit{floating-point operations per second} (FLOPS) yang lebih besar \parencite{unipelt}.
