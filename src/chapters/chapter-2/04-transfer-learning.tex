\section{\textit{Transfer Learning}}

\textit{Transfer Learning} merupakan salah satu pendekatan kunci dalam pembelajaran mesin yang memanfaatkan model yang telah dilatih pada tugas tertentu sebagai dasar untuk melatih model pada tugas lain. Ide dasar di balik \textit{transfer learning} adalah bahwa, jika model telah mempelajari fitur-fitur tertentu dari satu tugas, fitur-fitur tersebut dapat digunakan sebagai informasi awal yang berguna untuk tugas lain.

Sebagai contoh, model yang telah dilatih untuk mengenali objek dalam gambar dapat memanfaatkan pengetahuannya tentang fitur visual, seperti tepi atau tekstur, saat dilatih untuk tugas pengenalan wajah. Meskipun tugas awal (mengenali objek) dan tugas kedua (pengenalan wajah) berbeda, ada sejumlah fitur visual yang relevan untuk kedua tugas tersebut.

Dalam konteks pemrosesan bahasa alami (NLP), \textit{transfer learning} sering digunakan untuk memanfaatkan \textit{Large Language Model} (LLM) yang telah dilatih pada korpus teks besar untuk tugas-tugas spesifik seperti \textit{sentiment analysis} atau \textit{named entity recognition}. Dengan memulai dari model yang telah memiliki pemahaman dasar tentang struktur dan semantik bahasa, proses pelatihan untuk tugas spesifik menjadi lebih cepat dan seringkali menghasilkan model yang lebih akurat dibandingkan dengan melatih model dari awal.

Keuntungan lain dari \textit{transfer learning} adalah efisiensi komputasi. Melatih model pembelajaran mesin dari awal, terutama model dengan banyak parameter, memerlukan sumber daya komputasi yang signifikan. Dengan menggunakan model yang telah dilatih sebagai titik awal, dapat menghemat waktu dan sumber daya komputasi, sambil mempertahankan atau bahkan meningkatkan kinerja model.

