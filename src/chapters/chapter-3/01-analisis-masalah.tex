\section{Analisis Persoalan}

Dalam era digital yang terus berkembang, IndoBERT muncul sebagai model NLP yang penting, dirancang khusus untuk mengatasi kompleksitas linguistik bahasa Indonesia. Model ini telah menunjukkan peningkatan signifikan dalam berbagai tugas NLP, seperti \textit{text classification},  \textit{named entity recognition} (NER), dan \textit{sentiment analysis}. Namun, meskipun efektivitasnya terbukti, IndoBERT masih menghadapi tantangan dalam hal efisiensi komputasi dan penggunaan sumber daya. Ini menjadi isu kritis, terutama di lingkungan dengan keterbatasan sumber daya komputasi.

Salah satu tantangan utama dalam penggunaan IndoBERT adalah kebutuhan sumber daya komputasi yang besar untuk proses \textit{fine-tuning}. Metode \textit{fine-tuning} tradisional, sementara efektif dalam menyesuaikan model untuk tugas spesifik, sering kali memerlukan investasi sumber daya yang signifikan. Hal ini tidak selalu praktis atau bahkan mungkin di lingkungan dengan keterbatasan sumber daya, membatasi aksesibilitas dan aplikasi model ini dalam berbagai skenario penggunaan.

Sebagai respons terhadap keterbatasan ini, muncul pendekatan \textit{parameter-efficient transfer learning} (PETL). PETL menawarkan solusi yang menjanjikan, memungkinkan model untuk menyesuaikan diri dengan tugas-tugas spesifik dengan menambahkan jumlah parameter yang relatif kecil. Pendekatan ini mempertahankan sebagian besar parameter model yang telah dilatih sebelumnya tetap tidak berubah, mengurangi waktu dan biaya komputasi yang dibutuhkan. Ini membuka jalan untuk penyesuaian model yang lebih cepat dan lebih fleksibel untuk aplikasi spesifik.

Meskipun ada berbagai teknik PETL seperti LoRA, \textit{Prefix-Tuning}, \textit{Tiny-Attention Adapter}, belum ada penelitian komprehensif yang membandingkan kinerja antar teknik ini dalam konteks IndoBERT. Setiap teknik memiliki karakteristik uniknya sendiri, dan pemahaman yang lebih dalam tentang efektivitas relatif mereka sangat penting. Penelitian ini bertujuan untuk menjembatani kesenjangan ini dengan menerapkan dan membandingkan berbagai teknik PETL pada IndoBERT.
