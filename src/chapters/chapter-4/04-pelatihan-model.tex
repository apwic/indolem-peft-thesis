\section{Pelatihan Model}
\label{sec:pelatihan-model}

Pelatihan model dilakukan pada lingkungan eksperimen yang telah disebutkan pada subbab \ref{sec:lingkungan-eksperimen}. Pelatihan dilakukan dengan \textit{bash script} yang  dijalankan pada \textit{shell} untuk memudahkan reka ulang dari eksperimen tersebut. Untuk setiap tugas evaluasi terdapat \textit{bash script} yang  menjalankan semua eksperimen, yaitu \textit{fine-tuning}, \methodPEFT.

\begin{figure}[h]
    \centering
    \caption{Struktur \textit{file} eksperimen}
    \label{fig:file-eksperimen}
    \begin{forest}
        for tree={
            font=\ttfamily,
            grow'=0,
            child anchor=west,
            parent anchor=south,
            anchor=west,
            calign=first,
            edge path={
                \noexpand\path [draw, \forestoption{edge}]
                (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
            },
            before typesetting nodes={
                if n=1
                    {insert before={[,phantom]}}
                    {}
            },
            fit=band,
            before computing xy={l=15pt},
        }
    [run\_sentiment.sh
        [script
            [run\_sentiment\_base.sh]
            [run\_sentiment\_lora.sh]
            [run\_sentiment\_pt.sh]
            [run\_sentiment\_seq\_bn.sh]
            [run\_sentiment\_unipelt.sh]
        ]
    ]
    \end{forest}
\end{figure}

Pada gambar \ref{fig:file-eksperimen}, dengan melakukan pemanggilan pada {\ttfamily run\_sentiment.sh}  menjalankan semua eksperimen yang ada pada folder {\ttfamily script}. Setiap \textit{script}  menjalankan eksperimen sesuai dari konfigurasi yang telah ditentukan pada \textit{script} tersebut. Konfigurasi yang ditentukan tersebut  diterima sebagai argumen pada kakas IndoLEM. Untuk setiap \textit{hyperparameter} yang digunakan untuk pelatihan dapat dilihat pada lampiran \ref{appendix:hyperparameter-train}.

Untuk pelatihan model NER terdapat dua \textit{dataset} yang  digunakan yaitu NERUGM dan NERUI. Sedangkan, untuk tugas \textit{sentiment analysis} dan \textit{summarization} menggunakan satu dataset. Selain itu, untuk tugas NER terdapat argumen "return entity level metrics" dengan nilai "true" yang berarti penilaian metriks evaluasinya  dinilai pada level entitas. Ini mengikuti eksperimen yang sebelumnya sudah dilakukan pada IndoLEM.

Terdapat perbedaan pada \textit{hyperparameter} yang digunakan untuk tugas \textit{summarization} karena argumen yang digunakan mengikuti eksperimen pada IndoT5 bukan pada IndoLEM. Terdapat juga argumen "predict with generate" dengan nilai "true" karena tugas \textit{summarization} perlu menghasilkan hasil ringkasan untuk dapat dievaluasi. Selain itu, nilai argumen "bf16" bernilai "true" karena presisi "bf16" dianggap bisa mempercepat proses pelatihan untuk model T5 yang digunakan. Pada tugas ini perlu menambahkan argumen "source prefix" yang bernilai "summarize: " khusus untuk penggunaan model T5 pada tugas \textit{summarization}.

\begin{table}[h]
    \centering
    \caption{\textit{Hyperparameter} metode PEFT}
    \label{table:hyperparameter-PEFT}
    \begin{tabular}{l|l|c}
        \toprule
        \textbf{Metode} & \textbf{Argumen} & \textbf{Nilai} \\
        \midrule
        LoRA & \texttt{rank} & [8, 16] \\
        Prefix Tuning & \texttt{prefix\_length} & [5, 50] \\
        Adapter & \texttt{reduction\_factor} & [64, 16] \\
        UniPELT & \multicolumn{1}{c|}{-}  & - \\
        \bottomrule
    \end{tabular}
\end{table}

Pada pelatihan dengan metode PEFT  menggunakan \textit{hyperparameter} pelatihan seperti pada lampiran \ref{appendix:hyperparameter-train}. Untuk setiap metode PEFT terdapat \textit{hyperparameter} yang sesuai dengan metode PEFT tersebut. Perlu dilakukan \textit{hyperparameter tuning} untuk \textit{hyperparameter} metode PEFT. Seperti yang bisa dilihat pada tabel \ref{table:hyperparameter-PEFT}, untuk metode LoRA dilakukan eksperimen dengan argumen \texttt{ranks} dengan nilai 8 dan 16. Lalu, untuk metode Prefix Tuning dilakukan eksperimen dengan argumen \texttt{prefix\_tuning} dengan nilai 5 dan 50. Adapter akan menggunakan argumen \texttt{reduction\_factor} dengan nilai 64 dan 16. Selain itu, UniPELT tidak menggunakan argumen tambahan, hal ini dilakukan karena metode tersebut memang spesifik diimplementasikan pada pustaka Adapters sesuai dengan penelitian terkaitnya.
