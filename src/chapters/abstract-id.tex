\clearpage
\chapter*{ABSTRAK}
\addcontentsline{toc}{chapter}{Abstract}

\begin{center}
    \center
    \begin{singlespace}
        \large\bfseries\MakeUppercase{\thetitle}
    
        \normalfont\normalsize
        By:
    
        \bfseries \theauthor
    \end{singlespace}
\end{center} 


\begin{singlespace}
    IndoBERT telah menjadi model bahasa pre-trained yang diakui sebagai \textit{state of the art} dalam berbagai evaluasi di \textit{IndoLEM}.
    Namun, performanya dalam berbagai tugas masih memiliki ruang untuk peningkatan. 
    Beberapa teknik \textit{transfer learning} yang efisien dari sisi parameter semakin populer 
    untuk fine-tuning model bahasa yang telah di-pretrain, seperti \textit{LoRA}, \textit{Prefix-Tuning}, \textit{Tiny-Attention Adapter}, dan \textit{Unified Parameter Transfer Learning}. 
    Teknik-teknik ini dapat meningkatkan performa model pre-trained tanpa memerlukan sumber daya sebanyak metode \textit{Fine Tuning} tradisional. 
    Dalam penelitian ini, proses tuning akan dilakukan dengan metode-metode tersebut untuk meningkatkan performa IndoBERT dalam berbagai tugas dan membandingkan efektivitas teknik yang diterapkan.
\end{singlespace}
\clearpage

\clearpage