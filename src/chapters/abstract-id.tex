\clearpage
\chapter*{ABSTRAK}
\addcontentsline{toc}{chapter}{Abstract}

\begin{center}
    \center
    \begin{singlespace}
        \large\bfseries\MakeUppercase{\thetitle}
    
        \normalfont\normalsize
        By:
    
        \bfseries \theauthor
    \end{singlespace}
\end{center} 


\begin{singlespace}
    IndoBERT telah menjadi model bahasa pre-trained yang diakui sebagai \textit{state of the art} dalam berbagai evaluasi di \textit{IndoLEM}.
    Namun, kinerjanya dalam berbagai tugas masih memiliki ruang untuk peningkatan. 
    Beberapa teknik \textit{transfer learning} yang efisien dari sisi parameter semakin populer 
    untuk fine-tuning model bahasa yang telah di-pretrain, seperti \textit{LoRA}, \textit{Prefix-Tuning}, \textit{Bottleneck Adapter}, dan \textit{Unified Parameter Transfer Learning}. 
    Teknik-teknik ini dapat meningkatkan kinerja model pre-trained tanpa memerlukan sumber daya sebanyak metode \textit{fine-tuning} tradisional. 
    Dalam penelitian ini, proses tuning akan dilakukan dengan metode-metode tersebut untuk meningkatkan kinerja IndoBERT dalam berbagai tugas dan membandingkan efektivitas teknik yang diterapkan.
\end{singlespace}
\clearpage

\clearpage
