\section{Latar Belakang}

Pada era digital saat ini, pengolahan bahasa alami (\textit{Natural Language Processing}, NLP) telah menjadi salah satu bidang yang sangat penting dan mengalami perkembangan pesat. NLP memungkinkan komputer untuk memahami, menginterpretasi, dan merespons bahasa manusia dengan cara yang bermakna. Salah satu model yang telah terbukti efektif dalam berbagai aplikasi NLP adalah BERT (\textit{Bidirectional Encoder Representations from Transformers}). IndoBERT, sebagai variasi dari model BERT yang dirancang khusus untuk bahasa Indonesia, telah menjadi pilihan utama dalam pengolahan bahasa alami berbahasa Indonesia.

Meskipun IndoBERT telah memberikan hasil yang baik dalam berbagai tugas NLP, masih ada potensi untuk meningkatkan kinerjanya lebih lanjut. Salah satu pendekatan yang muncul untuk meningkatkan kinerja model bahasa seperti IndoBERT adalah melalui penggunaan teknik \textit{parameter-efficient transfer learning}. Teknik ini memungkinkan model untuk mempelajari tugas-tugas spesifik dengan jumlah parameter yang lebih sedikit, yang pada gilirannya dapat meningkatkan efisiensi penggunaan sumber daya, terutama parameter model.

Dalam konteks ini, teknik \textit{parameter-efficient transfer learning} memiliki berbagai variasi yang menarik, seperti LoRA (\textit{Low-Rank Adaptation}), \textit{Prefix-Tuning}, \textit{Tiny-Attention Adapter}, dan \textit{Unified View of Parameter-Efficient Transfer Learning}. Setiap teknik memiliki karakteristiknya sendiri, dan hingga saat ini, belum ada banyak penelitian yang secara komprehensif membandingkan kinerja antara teknik-teknik ini, terutama ketika diterapkan pada model IndoBERT.

Penelitian ini akan difokuskan pada penerapan berbagai teknik \textit{parameter-efficient transfer learning} pada model IndoBERT dengan tujuan mengukur sejauh mana teknik-teknik ini dapat meningkatkan kinerja model dalam berbagai tugas NLP. Selain itu, penelitian ini akan membandingkan kinerja berbagai teknik \textit{parameter-efficient transfer learning} dengan metode \textit{fine-tuning} tradisional. Fokus utama penelitian ini adalah bagaimana efisiensi penggunaan sumber daya, terutama parameter model, dapat mempengaruhi peningkatan kinerja model bahasa IndoBERT.

Dengan demikian, penelitian ini diharapkan dapat memberikan kontribusi dalam pengembangan metode untuk meningkatkan kinerja model bahasa IndoBERT, sekaligus memberikan pemahaman yang lebih baik mengenai efektivitas dan efisiensi berbagai teknik \textit{parameter-efficient transfer learning} dalam konteks bahasa Indonesia.