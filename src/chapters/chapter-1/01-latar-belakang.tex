\section{Latar Belakang}

Pada era digital saat ini, pengolahan bahasa alami (\textit{Natural Language Processing}, NLP) menjadi sebuah bidang yang sangat penting dan berkembang pesat. NLP memungkinkan komputer untuk memahami, menginterpretasi, dan merespons bahasa manusia dengan cara yang bermakna. Salah satu model yang sering digunakan dalam berbagai aplikasi NLP adalah BERT (\textit{Bidirectional Encoder Representations from Transformers}). IndoBERT merupakan salah satu variasi dari model BERT yang telah dilatih khusus untuk bahasa Indonesia.

Walaupun IndoBERT telah menunjukkan performa yang baik dalam berbagai tugas NLP, terdapat potensi untuk meningkatkan performanya lebih lanjut. Salah satu cara untuk meningkatkan performa model bahasa seperti IndoBERT adalah dengan menggunakan teknik \textit{parameter-efficient transfer learning}. Teknik ini memungkinkan model untuk mempelajari tugas-tugas spesifik dengan jumlah parameter yang lebih sedikit, sehingga lebih efisien dalam hal penggunaan sumber daya.

Teknik \textit{parameter-efficient transfer learning} sendiri memiliki berbagai variasi, seperti LoRA (\textit{Low-Rank Adaptation}), \textit{Prefix-Tuning}, \textit{Tiny-Attention Adapter}, dan \textit{Unified View of Parameter-Efficient Transfer Learning}. Masing-masing teknik ini memiliki karakteristiknya tersendiri dan belum banyak penelitian yang mengkaji perbandingan kinerja antar teknik tersebut khususnya pada model IndoBERT.

Dalam konteks ini, penelitian ini akan fokus pada pengaplikasian berbagai teknik \textit{parameter-efficient transfer learning} pada model IndoBERT untuk melihat sejauh mana teknik-teknik tersebut dapat meningkatkan performa model dalam berbagai tugas NLP. Selain itu, penelitian ini juga akan mengkaji perbedaan kinerja antar teknik \textit{parameter-efficient transfer learning} dan membandingkannya dengan metode \textit{fine-tuning} tradisional.

Dengan demikian, penelitian ini diharapkan dapat memberikan kontribusi dalam pengembangan metode untuk meningkatkan performa model bahasa IndoBERT, sekaligus memberikan pemahaman yang lebih baik mengenai efektivitas dan efisiensi berbagai teknik \textit{parameter-efficient transfer learning}.