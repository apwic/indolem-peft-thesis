\section{Latar Belakang}
\label{sec:latar-belakang}

% TODO: change to match the title
Pengolahan bahasa alami (\textit{Natural Language Processing}, NLP) telah menjadi salah satu bidang yang mengalami perkembangan pesat. Kemampuan untuk memahami, menginterpretasi, dan merespons bahasa manusia secara bermakna telah membuka berbagai aplikasi baru yang inovatif. Pengaplikasian bahasa alami mencakup berbagai aspek, mulai dari pemahaman teks (seperti \textit{text classification} dan \textit{sentiment analysis}), hingga generasi bahasa (seperti \textit{machine translation}).

Model BERT (\textit{Bidirectional Encoder Representations from Transformers}) telah menjadi terobosan dalam NLP, menjadi model \textit{state-of-the-art} dalam berbagai jenis tugas pemrosesan bahasa alami \parencite{bert}. BERT merupakan \textit{pre-trained model} yang dapat diterapkan \textit{transfer-learning} untuk mengadaptasi model tersebut pada tugas pemrosesan bahasa alami lainnya. Sebagai adaptasi dari BERT, IndoBERT dirancang untuk mengatasi tantangan unik yang ditimbulkan oleh kekayaan linguistik bahasa Indonesia. Menunjukkan kinerja \textit{state-of-the-art} dalam \textit{benchmark} evaluasi untuk tugas pemrosesan bahasa alami berbahasa Indonesia \parencite{indobert}.

Meskipun dengan kinerja IndoBERT yang sudah mencapai \textit{state-of-the-art} dalam bahasa Indonesia, terdapat peluang untuk peningkatan lebih lanjut, terutama dalam hal efisiensi parameter yang digunakan dan penggunaan sumber daya komputasi. Penggunaan \textit{transfer-learning}, salah satunya dengan melakukan \textit{fine-tuning} model seperti IndoBERT, memerlukan sumber daya komputasi yang besar, yang tidak selalu tersedia atau praktis untuk semua pengguna. Salah satu pendekatan yang menjanjikan untuk mengatasi tantangan ini adalah penggunaan teknik \PEFT (PEFT). PEFT memungkinkan model untuk menyesuaikan dengan tugas-tugas spesifik dengan mengubah jumlah parameter yang relatif kecil sehingga mempertahankan sebagian besar parameter model yang telah dilatih sebelumnya \parencite{adapter}. Teknik ini tidak hanya dapat mengurangi waktu dan biaya komputasi, tetapi juga memungkinkan penyesuaian model yang lebih cepat dan lebih fleksibel untuk aplikasi spesifik. 

Teknik \PEFT (PEFT) memiliki berbagai variasi, seperti LoRA (\textit{Low-Rank Adaptation}), \textit{Prefix-Tuning}, dan \textit{Bottleneck Adapter}. Penelitian yang dilakukan oleh \citeauthor{adapter} \parencite{adapter} dan \citeauthor{uvpl} \parencite{uvpl} menunjukkan bahwa dengan memperbarui kurang dari 4\% parameter, metode berbasis PEFT dapat mencapai kinerja yang sebanding dengan metode \textit{fine-tuning} penuh. Setiap teknik memiliki karakteristiknya sendiri, dan hingga saat ini, belum banyak penelitian yang secara komprehensif membandingkan kinerja antara teknik-teknik ini, terutama ketika diterapkan pada model IndoBERT. 

Penelitian ini akan difokuskan pada penerapan berbagai teknik \PEFT (PEFT) pada model IndoBERT dengan tujuan mengukur sejauh mana teknik-teknik ini dapat meningkatkan kinerja model dalam berbagai tugas NLP, yaitu \textit{dependency parsing}, \textit{named entity recognition}, dan \textit{sentiment analysis}. Selain itu, penelitian ini akan membandingkan kinerja berbagai teknik \PEFT dengan metode \textit{fine-tuning} tradisional untuk mendapatkan metode mana yang paling efisien. Fokus utama penelitian ini adalah bagaimana efisiensi penggunaan sumber daya, terutama parameter model, dapat mempengaruhi peningkatan kinerja model bahasa IndoBERT.

Dengan demikian, penelitian ini diharapkan dapat memberikan kontribusi dalam pengembangan metode untuk meningkatkan kinerja model bahasa IndoBERT, sekaligus memberikan pemahaman yang lebih baik mengenai efektivitas dan efisiensi berbagai teknik \PEFT dalam konteks bahasa Indonesia.
