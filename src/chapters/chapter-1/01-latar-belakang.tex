\section{Latar Belakang}

Pada era digital saat ini, pengolahan bahasa alami (\textit{Natural Language Processing}, NLP) telah menjadi salah satu bidang yang sangat penting dan mengalami perkembangan pesat.  Kemampuan untuk memahami, menginterpretasi, dan merespons bahasa manusia secara bermakna telah membuka berbagai aplikasi baru dan inovatif. Model BERT (\textit{Bidirectional Encoder Representations from Transformers}) telah menjadi terobosan dalam NLP, memberikan peningkatan signifikan dalam pemahaman konteks bahasa. Sebagai adaptasi dari BERT, IndoBERT telah dirancang untuk mengatasi tantangan unik yang ditimbulkan oleh kekayaan linguistik bahasa Indonesia, menunjukkan peningkatan performa dalam berbagai tugas NLP seperti \textit{text classification},  \textit{named entity recognition} (NER), dan \textit{sentiment analysis}.

Meskipun IndoBERT telah memberikan hasil yang mengesankan, terdapat peluang untuk peningkatan lebih lanjut, terutama dalam hal efisiensi komputasi dan penggunaan sumber daya. \textit{Fine-tuning} model seperti IndoBERT seringkali memerlukan sumber daya komputasi yang besar, yang tidak selalu tersedia atau praktis untuk semua pengguna, terutama di lingkungan dengan keterbatasan sumber daya komputasi. Sebagai alternatif, \cite{parameter} mengeksplorasi penggunaan \textit{adapter modules} yang menambahkan hanya sedikit parameter yang dapat dilatih per tugas, memungkinkan penyesuaian model yang lebih \textit{paramter-efficient} tanpa mengorbankan kinerja.

Salah satu pendekatan yang menjanjikan untuk mengatasi tantangan ini adalah penggunaan teknik \textit{parameter-efficient transfer learning} (PETL). PETL memungkinkan model untuk menyesuaikan diri dengan tugas-tugas spesifik dengan menambahkan jumlah parameter yang relatif kecil, sehingga mempertahankan sebagian besar parameter model yang telah dilatih sebelumnya tetap tidak berubah. Teknik ini tidak hanya dapat mengurangi waktu dan biaya komputasi tetapi juga memungkinkan penyesuaian model yang lebih cepat dan lebih fleksibel untuk aplikasi spesifik. \cite{uvpl} memberikan kerangka kerja terpadu yang menetapkan hubungan antara berbagai metode PETL, yang dapat membantu dalam memahami bahan kunci untuk kesuksesan mereka dalam berbagai tugas NLP.

Dalam konteks ini, teknik \textit{parameter-efficient transfer learning} (PETL) memiliki berbagai variasi yang menarik, seperti LoRA (\textit{Low-Rank Adaptation}), \textit{Prefix-Tuning}, \textit{Tiny-Attention Adapter}, dan \textit{Unified View of Parameter-Efficient Transfer Learning}. Setiap teknik memiliki karakteristiknya sendiri, dan hingga saat ini, belum ada banyak penelitian yang secara komprehensif membandingkan kinerja antara teknik-teknik ini, terutama ketika diterapkan pada model IndoBERT. Studi oleh \cite{parameter} dan \cite{uvpl} menunjukkan bahwa dengan memperbarui kurang dari 4\% parameter, metode berbasis PETL dapat mencapai kinerja yang sebanding dengan metode fine-tuning penuh.

Penelitian ini akan difokuskan pada penerapan berbagai teknik \textit{parameter-efficient transfer learning} pada model IndoBERT dengan tujuan mengukur sejauh mana teknik-teknik ini dapat meningkatkan kinerja model dalam berbagai tugas NLP. Selain itu, penelitian ini akan membandingkan kinerja berbagai teknik \textit{parameter-efficient transfer learning} dengan metode \textit{fine-tuning} tradisional. Fokus utama penelitian ini adalah bagaimana efisiensi penggunaan sumber daya, terutama parameter model, dapat mempengaruhi peningkatan kinerja model bahasa IndoBERT.

Dengan demikian, penelitian ini diharapkan dapat memberikan kontribusi dalam pengembangan metode untuk meningkatkan kinerja model bahasa IndoBERT, sekaligus memberikan pemahaman yang lebih baik mengenai efektivitas dan efisiensi berbagai teknik \textit{parameter-efficient transfer learning} dalam konteks bahasa Indonesia.