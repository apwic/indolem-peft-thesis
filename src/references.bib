% NLP
@book{ai,
	added-at = {2020-02-01T18:23:11.000+0100},
	author = {Russell, Stuart and Norvig, Peter},
	biburl = {
	          https://www.bibsonomy.org/bibtex/20533b732950d1c5ab4ac12d4f32fe637/mialhoma
	          },
	edition = 3,
	interhash = {53908a52dd4c6c8e39f93f4ffc8341be},
	intrahash = {0533b732950d1c5ab4ac12d4f32fe637},
	keywords = {ties4530},
	publisher = {Prentice Hall},
	timestamp = {2020-02-01T18:23:11.000+0100},
	title = {Artificial Intelligence: A Modern Approach},
	year = 2010,
}

@book{nlp,
	author = {Jurafsky, Daniel and Martin, James},
	year = {2008},
	month = {02},
	pages = {},
	title = {Speech and Language Processing: An Introduction to Natural
	         Language Processing, Computational Linguistics, and Speech
	         Recognition},
	volume = {2},
}

% NLP Task
@inproceedings{sentiment_stock,
	author = {Cakra, Yahya Eru and Distiawan Trisedya, Bayu},
	booktitle = {2015 International Conference on Advanced Computer Science
	             and Information Systems (ICACSIS)},
	title = {Stock price prediction using linear regression based on
	         sentiment analysis},
	year = {2015},
	volume = {},
	number = {},
	pages = {147-154},
	keywords = {Companies;Classification algorithms;Sentiment
	            analysis;Twitter;Support vector machines;Linear
	            regression;Vegetation;linear regression;sentiment
	            analysis;stock price;supervised learning;Twitter},
	doi = {10.1109/ICACSIS.2015.7415179},
}


@article{sentiment_algo,
	abstract = {Sentiment Analysis (SA) is an ongoing field of research in
	            text mining field. SA is the computational treatment of
	            opinions, sentiments and subjectivity of text. This survey
	            paper tackles a comprehensive overview of the last update in
	            this field. Many recently proposed algorithms' enhancements
	            and various SA applications are investigated and presented
	            briefly in this survey. These articles are categorized
	            according to their contributions in the various SA
	            techniques. The related fields to SA (transfer learning,
	            emotion detection, and building resources) that attracted
	            researchers recently are discussed. The main target of this
	            survey is to give nearly full image of SA techniques and the
	            related fields with brief details. The main contributions of
	            this paper include the sophisticated categorizations of a
	            large number of recent articles and the illustration of the
	            recent trend of research in the sentiment analysis and its
	            related areas.},
	author = {Walaa Medhat and Ahmed Hassan and Hoda Korashy},
	doi = {https://doi.org/10.1016/j.asej.2014.04.011},
	issn = {2090-4479},
	journal = {Ain Shams Engineering Journal},
	keywords = {Sentiment analysis, Sentiment classification, Feature
	            selection, Emotion detection, Transfer learning, Building
	            resources},
	number = {4},
	pages = {1093-1113},
	title = {Sentiment analysis algorithms and applications: A survey},
	url = {
	       https://www.sciencedirect.com/science/article/pii/S2090447914000550
	       },
	volume = {5},
	year = {2014},
	bdsk-url-1 = {
	              https://www.sciencedirect.com/science/article/pii/S2090447914000550
	              },
	bdsk-url-2 = {https://doi.org/10.1016/j.asej.2014.04.011},
}


@inproceedings{ner,
	author = {Azarine, Indira Suri and Arif Bijaksana, Moch and Asror, Ibnu},
	booktitle = {2019 7th International Conference on Information and
	             Communication Technology (ICoICT)},
	title = {Named Entity Recognition on Indonesian Tweets using Hidden
	         Markov Model},
	year = {2019},
	volume = {},
	number = {},
	pages = {1-5},
	keywords = {Hidden Markov
	            models;Twitter;Organizations;Tokenization;Labeling;Stochastic
	            processes;Grammar;Named Entity Recognition;Tweet;Hidden
	            Markov Model.},
	doi = {10.1109/ICoICT.2019.8835277},
}

@misc{summarization,
	title = {IndoSum: A New Benchmark Dataset for Indonesian Text
	         Summarization},
	author = {Kemal Kurniawan and Samuel Louvan},
	year = {2019},
	eprint = {1810.05334},
	archivePrefix = {arXiv},
	primaryClass = {cs.CL},
	url = {https://arxiv.org/abs/1810.05334},
}

% Models

@inproceedings{transformers,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
	          Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser
	          , \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	title = {Attention is All you Need},
	url = {
	       https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
	       },
	volume = {30},
	year = {2017},
	bdsk-url-1 = {
	              https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
	              },
}

@inproceedings{bert,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for
	         Language Understanding",
	author = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
	          Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican
	             Chapter of the Association for Computational Linguistics:
	             Human Language Technologies, Volume 1 (Long and Short
	             Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT
	            , which stands for Bidirectional Encoder Representations from
	            Transformers. Unlike recent language representation models
	            (Peters et al., 2018a; Radford et al., 2018), BERT is
	            designed to pre-train deep bidirectional representations from
	            unlabeled text by jointly conditioning on both left and right
	            context in all layers. As a result, the pre-trained BERT
	            model can be fine-tuned with just one additional output layer
	            to create state-of-the-art models for a wide range of tasks,
	            such as question answering and language inference, without
	            substantial task-specific architecture modifications. BERT is
	            conceptually simple and empirically powerful. It obtains new
	            state-of-the-art results on eleven natural language
	            processing tasks, including pushing the GLUE score to 80.5
	            (7.7 point absolute improvement), MultiNLI accuracy to 86.7{
	            \%} (4.6{\%} absolute improvement), SQuAD v1.1 question
	            answering Test F1 to 93.2 (1.5 point absolute improvement)
	            and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute
	            improvement).",
}

@article{T5,
	author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine
	          Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei
	          Li and Peter J. Liu},
	title = {Exploring the Limits of Transfer Learning with a Unified
	         Text-to-Text Transformer},
	journal = {Journal of Machine Learning Research},
	year = {2020},
	volume = {21},
	number = {140},
	pages = {1--67},
	url = {http://jmlr.org/papers/v21/20-074.html},
}

@online{indoT5,
	author = {LazarusNLP},
	title = {IndoT5: Indonesian Language Model},
	year = {2024},
	url = {https://github.com/LazarusNLP/IndoT5},
	urldate = {2024-07-24},
}

% Adapters
@article{lora,
	title = {Low-Rank Adaptation for Large Language Models},
	author = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan
	          Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu
	          Chen },
	year = {2021},
	journal = {arXiv preprint arXiv:2106.09685},
	url = {https://arxiv.org/abs/2106.09685},
}

@inproceedings{adapter_tiny_attention,
	title = "Tiny-Attention Adapter: Contexts Are More Important Than the
	         Number of Parameters",
	author = "Zhao, Hongyu and Tan, Hao and Mei, Hongyuan",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in
	             Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.444",
	doi = "10.18653/v1/2022.emnlp-main.444",
	pages = "6626--6638",
	abstract = "Adapter-tuning is a paradigm that transfers a pretrained
	            language model to downstream tasks by adding and tuning a
	            small number of new parameters. Previously proposed adapter
	            architectures are all feed-forward neural networks. In this
	            paper, we investigate the effectiveness of using
	            tiny-attention{---}i.e., attention with extremely small
	            per-head dimensionality{---}as adapters. Our tiny-attention
	            adapter learns to modify the hidden states at each position
	            directly conditioned on the hidden states at all the other
	            positions, which is missed by the previously proposed
	            adapters. Moreover, we view its multiple attention heads as a
	            mixture of experts and propose to average their weights
	            during deployment, which further reduces its inference
	            computation cost. On the GLUE benchmark, our tiny-attention
	            adapter outperforms the other parameter-efficient transfer
	            learning methods as well as full fine-tuning while only
	            updating 0.05{\%} of the parameters. On the FewGLUE benchmark
	            , its performance is comparable to that of GPT-3 and PET.",
}

@inproceedings{prefix_tuning,
	title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
	author = "Li, Xiang Lisa and Liang, Percy",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association
	             for Computational Linguistics and the 11th International
	             Joint Conference on Natural Language Processing (Volume 1:
	             Long Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-long.353",
	doi = "10.18653/v1/2021.acl-long.353",
	pages = "4582--4597",
	abstract = "Fine-tuning is the de facto way of leveraging large
	            pretrained language models for downstream tasks. However,
	            fine-tuning modifies all the language model parameters and
	            therefore necessitates storing a full copy for each task. In
	            this paper, we propose prefix-tuning, a lightweight
	            alternative to fine-tuning for natural language generation
	            tasks, which keeps language model parameters frozen and
	            instead optimizes a sequence of continuous task-specific
	            vectors, which we call the prefix. Prefix-tuning draws
	            inspiration from prompting for language models, allowing
	            subsequent tokens to attend to this prefix as if it were {``}
	            virtual tokens{''}. We apply prefix-tuning to GPT-2 for
	            table-to-text generation and to BART for summarization. We
	            show that by learning only 0.1{\%} of the parameters,
	            prefix-tuning obtains comparable performance in the full data
	            setting, outperforms fine-tuning in low-data settings, and
	            extrapolates better to examples with topics that are unseen
	            during training.",
}

@inproceedings{adapter_houlsby,
	title = {Parameter-Efficient Transfer Learning for NLP},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw
	          and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo,
	          Andrea and Attariyan, Mona and Gelly, Sylvain},
	booktitle = {Proceedings of the 36th International Conference on Machine
	             Learning},
	pages = {2790--2799},
	year = {2019},
	volume = {97},
	series = {Proceedings of Machine Learning Research},
	month = {09--15 Jun},
	publisher = {PMLR},
	pdf = {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
	url = {https://proceedings.mlr.press/v97/houlsby19a.html},
}

@inproceedings{adapter_pfeiffer,
	title = "{A}dapter{F}usion: Non-Destructive Task Composition for
	         Transfer Learning",
	author = {Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e},
	          Andreas and Cho, Kyunghyun and Gurevych, Iryna},
	booktitle = "Proceedings of the 16th Conference of the European Chapter
	             of the Association for Computational Linguistics: Main
	             Volume",
	month = apr,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.eacl-main.39",
	doi = "10.18653/v1/2021.eacl-main.39",
	pages = "487--503",
	abstract = "Sequential fine-tuning and multi-task learning are methods
	            aiming to incorporate knowledge from multiple tasks; however,
	            they suffer from catastrophic forgetting and difficulties in
	            dataset balancing. To address these shortcomings, we propose
	            AdapterFusion, a new two stage learning algorithm that
	            leverages knowledge from multiple tasks. First, in the
	            knowledge extraction stage we learn task specific parameters
	            called adapters, that encapsulate the task-specific
	            information. We then combine the adapters in a separate
	            knowledge composition step. We show that by separating the
	            two stages, i.e., knowledge extraction and knowledge
	            composition, the classifier can effectively exploit the
	            representations learned from multiple tasks in a
	            non-destructive manner. We empirically evaluate AdapterFusion
	            on 16 diverse NLU tasks, and find that it effectively
	            combines various types of knowledge at different layers of
	            the model. We show that our approach outperforms traditional
	            strategies such as full fine-tuning as well as multi-task
	            learning. Our code and adapters are available at
	            AdapterHub.ml.",
}

@inproceedings{prompt_tuning,
	title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
	author = "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
	             Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.243",
	doi = "10.18653/v1/2021.emnlp-main.243",
	pages = "3045--3059",
	abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet
	            effective mechanism for learning {``}soft prompts{''} to
	            condition frozen language models to perform specific
	            downstream tasks. Unlike the discrete text prompts used by
	            GPT-3, soft prompts are learned through backpropagation and
	            can be tuned to incorporate signals from any number of
	            labeled examples. Our end-to-end learned approach outperforms
	            GPT-3{'}s few-shot learning by a large margin. More
	            remarkably, through ablations on model size using T5, we show
	            that prompt tuning becomes more competitive with scale: as
	            models exceed billions of parameters, our method {``}closes
	            the gap{''} and matches the strong performance of model
	            tuning (where all model weights are tuned). This finding is
	            especially relevant because large models are costly to share
	            and serve and the ability to reuse one frozen model for
	            multiple downstream tasks can ease this burden. Our method
	            can be seen as a simplification of the recently proposed {``}
	            prefix tuning{''} of Li and Liang (2021) and we provide a
	            comparison to this and other similar approaches. Finally, we
	            show that conditioning a frozen model with soft prompts
	            confers benefits in robustness to domain transfer and enables
	            efficient {``}prompt ensembling.{''} We release code and
	            model checkpoints to reproduce our experiments.",
}

@inproceedings{compacter,
	author = {Karimi Mahabadi, Rabeeh and Henderson, James and Ruder,
	          Sebastian},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {1022--1035},
	publisher = {Curran Associates, Inc.},
	title = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
	url = {
	       https://proceedings.neurips.cc/paper_files/paper/2021/file/081be9fdff07f3bc808f935906ef70c0-Paper.pdf
	       },
	volume = {34},
	year = {2021},
	bdsk-url-1 = {
	              https://proceedings.neurips.cc/paper_files/paper/2021/file/081be9fdff07f3bc808f935906ef70c0-Paper.pdf
	              },
}

@inproceedings{ia3,
	author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta,
	          Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {1950--1965},
	publisher = {Curran Associates, Inc.},
	title = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper
	         than In-Context Learning},
	url = {
	       https://proceedings.neurips.cc/paper_files/paper/2022/file/0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf
	       },
	volume = {35},
	year = {2022},
	bdsk-url-1 = {
	              https://proceedings.neurips.cc/paper_files/paper/2022/file/0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf
	              },
}


% LEM
@inproceedings{glue,
	title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
	         Language Understanding},
	url = {http://dx.doi.org/10.18653/v1/W18-5446},
	DOI = {10.18653/v1/w18-5446},
	booktitle = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
	             Analyzing and Interpreting Neural Networks for NLP},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill,
	          Felix and Levy, Omer and Bowman, Samuel},
	year = {2018},
}

@misc{superglue,
	title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language
	         Understanding Systems},
	author = {Alex Wang and Yada Pruksachatkun and Nikita Nangia and
	          Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy
	          and Samuel R. Bowman},
	year = {2019},
	eprint = {1905.00537},
	archivePrefix = {arXiv},
	primaryClass = {cs.CL},
}

@misc{indonlu,
	title = {IndoNLU: Benchmark and Resources for Evaluating Indonesian
	         Natural Language Understanding},
	author = {Bryan Wilie and Karissa Vincentio and Genta Indra Winata and
	          Samuel Cahyawijaya and Xiaohong Li and Zhi Yuan Lim and Sidik
	          Soleman and Rahmad Mahendra and Pascale Fung and Syafri Bahar
	          and Ayu Purwarianti},
	year = {2020},
	eprint = {2009.05387},
	archivePrefix = {arXiv},
	primaryClass = {cs.CL},
}

@inproceedings{indonlg,
	title = {IndoNLG: Benchmark and Resources for Evaluating Indonesian
	         Natural Language Generation},
	url = {http://dx.doi.org/10.18653/v1/2021.emnlp-main.699},
	DOI = {10.18653/v1/2021.emnlp-main.699},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in
	             Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Cahyawijaya, Samuel and Winata, Genta Indra and Wilie, Bryan
	          and Vincentio, Karissa and Li, Xiaohong and Kuncoro, Adhiguna
	          and Ruder, Sebastian and Lim, Zhi Yuan and Bahar, Syafri and
	          Khodra, Masayu and Purwarianti, Ayu and Fung, Pascale},
	year = {2021},
}

@inproceedings{indolem,
	title = "{I}ndo{LEM} and {I}ndo{BERT}: A Benchmark Dataset and
	         Pre-trained Language Model for {I}ndonesian {NLP}",
	author = "Koto, Fajri and Rahimi, Afshin and Lau, Jey Han and Baldwin,
	          Timothy",
	booktitle = "Proceedings of the 28th International Conference on
	             Computational Linguistics",
	month = dec,
	year = "2020",
	address = "Barcelona, Spain (Online)",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2020.coling-main.66",
	doi = "10.18653/v1/2020.coling-main.66",
	pages = "757--770",
	abstract = "Although the Indonesian language is spoken by almost 200
	            million people and the 10th most spoken language in the world
	            , it is under-represented in NLP research. Previous work on
	            Indonesian has been hampered by a lack of annotated datasets,
	            a sparsity of language resources, and a lack of resource
	            standardization. In this work, we release the IndoLEM dataset
	            comprising seven tasks for the Indonesian language, spanning
	            morpho-syntax, semantics, and discourse. We additionally
	            release IndoBERT, a new pre-trained language model for
	            Indonesian, and evaluate it over IndoLEM, in addition to
	            benchmarking it against existing resources. Our experiments
	            show that IndoBERT achieves state-of-the-art performance over
	            most of the tasks in IndoLEM.",
}

% Metrics
@inbook{metrics,
	author = {Dalianis, Hercules},
	year = {2018},
	month = {05},
	pages = {45-53},
	title = {Evaluation Metrics and Evaluation},
	isbn = {978-3-319-78502-8},
	doi = {10.1007/978-3-319-78503-5_6},
}

% Related Research
@article{peft_on_plm,
	author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and
	          Yang , Zonghan and Su, Yusheng and Hu, Shengding and Chen,
	          Yulin and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao,
	          Weilin and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao
	          and Chen, Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi
	          and Sun, Maosong},
	date-added = {2023-12-14 01:30:09 +0700},
	date-modified = {2023-12-14 01:30:09 +0700},
	doi = {10.1038/s42256-023-00626-4},
	id = {Ding2023},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {3},
	pages = {220--235},
	title = {Parameter-efficient fine-tuning of large-scale pre-trained
	         language models},
	url = {https://doi.org/10.1038/s42256-023-00626-4},
	volume = {5},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-023-00626-4},
}

@misc{uvpl,
	title = {Towards a Unified View of Parameter-Efficient Transfer Learning
	         },
	author = {Junxian He and Chunting Zhou and Xuezhe Ma and Taylor
	          Berg-Kirkpatrick and Graham Neubig},
	year = {2022},
	eprint = {2110.04366},
	archivePrefix = {arXiv},
	primaryClass = {cs.CL},
	url = {https://arxiv.org/abs/2110.04366},
}

@inproceedings{adapters,
	title = "Adapters: A Unified Library for Parameter-Efficient and Modular
	         Transfer Learning",
	author = {Poth, Clifton and Sterz, Hannah and Paul, Indraneil and
	          Purkayastha, Sukannya and Engl{\"a}nder, Leon and Imhof, Timo
	          and Vuli{\'c}, Ivan and Ruder, Sebastian and Gurevych, Iryna
	          and Pfeiffer, Jonas},
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in
	             Natural Language Processing: System Demonstrations",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-demo.13",
	doi = "10.18653/v1/2023.emnlp-demo.13",
	pages = "149--160",
	abstract = "We introduce Adapters, an open-source library that unifies
	            parameter-efficient and modular transfer learning in large
	            language models. By integrating 10 diverse adapter methods
	            into a unified interface, Adapters offers ease of use and
	            flexible configuration. Our library allows researchers and
	            practitioners to leverage adapter modularity through
	            composition blocks, enabling the design of complex adapter
	            setups. We demonstrate the library{'}s efficacy by evaluating
	            its performance against full fine-tuning on various NLP
	            tasks. Adapters provides a powerful tool for addressing the
	            challenges of conventional fine-tuning paradigms and
	            promoting more efficient and modular transfer learning. The
	            library is available via https://adapterhub.ml/adapters.",
}
